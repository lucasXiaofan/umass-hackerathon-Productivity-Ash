# Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B

## Basic Info
- **Authors**: Sen Xu, Yi Zhou, Wei Wang, Jixin Min, Zhibin Yin, Yingwei Dai, Shixi Liu, Lianyu Pang, Yirong Chen, Junlin Zhang
- **Affiliation**: Sina Weibo Inc.
- **Date**: Nov. 7, 2025
- **GitHub**: https://github.com/WeiboAI/VibeThinker
- **HuggingFace**: https://huggingface.co/WeiboAI/VibeThinker-1.5B

## BibTeX Citation
@misc{xu2025tinymodelbiglogic,
  title={Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B},
  author={Sen Xu and Yi Zhou and Wei Wang and Jixin Min and Zhibin Yin and Yingwei Dai and Shixi Liu and Lianyu Pang and Yirong Chen and Junlin Zhang},
  year={2025},
  eprint={2511.06221},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2511.06221},
}

## Summary
This paper introduces VibeThinker-1.5B, a 1.5B-parameter dense model that demonstrates superior reasoning capabilities compared to larger models like Magistral Medium and Claude Opus 4, while achieving performance on par with open-source models like GPT OSS-20B Medium. The model uses an innovative post-training technique centered on the “Spectrum-to-Signal Principle (SSP)” and has a total training cost of only ,800.

## Notes
- The paper challenges the assumption that small models inherently lack robust reasoning capabilities.
- VibeThinker-1.5B outperforms many significantly larger models on challenging mathematical and coding benchmarks.

---
*This paper is ready for your review. Please let me know if you need any additional information or adjustments.*
