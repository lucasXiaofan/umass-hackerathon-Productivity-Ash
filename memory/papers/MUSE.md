# MUSE: Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks

## Metadata
- **Title**: Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks
- **Authors**: Cheng Yang, Xuemeng Yang, Licheng Wen, Daocheng Fu, Jianbiao Mei, Rong Wu, Pinlong Cai, Yufan Shen, Nianchen Deng, Botian Shi, Yu Qiao, Haifeng Li
- **Year**: 2025
- **Venue**: arXiv preprint arXiv:2510.08002
- **URL**: https://arxiv.org/abs/2510.08002
- **BibTeX**:
```
@article{yang2025learning,
  title={Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks},
  author={Yang, Cheng and Yang, Xuemeng and Wen, Licheng and Fu, Daocheng and Mei, Jianbiao and Wu, Rong and Cai, Pinlong and Shen, Yufan and Deng, Nianchen and Shi, Botian and Qiao, Yu and Li, Haifeng},
  journal={arXiv preprint arXiv:2510.08002},
  year={2025}
}
```

## Abstract
Large Language Models have demonstrated remarkable capabilities across diverse domains, yet significant challenges persist when deploying them as AI agents for real-world long-horizon tasks. Existing LLM agents suffer from a critical limitation: they are test-time static and cannot learn from their interactions with the environment. To address persistent challenges in dynamic planning, experience accumulation, and continuous learning for existing agents, we propose a novel agent framework MUSE, which stands for Memory-Utilizing and Self-Evolving. As illustrated in Figure 1, the core of MUSE is an experience-driven, closed-loop system centered around a Memory Module. This module hierarchically organizes diverse levels of knowledge, including procedural knowledge, strategic patterns, and tool-use guidance. Operating within a cross-application interactive environment, the agent leverages its accumulated experience to plan and explore solutions for long-horizon productivity tasks. After each sub-task execution, the agent autonomously reflects on its trajectory, converting the raw trajectory into structured experience and integrating it back into the Memory Module to enhance future performance.

## GitHub Repository
No GitHub repository found.

## Notes

